{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import time\n",
    "import codecs\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import scipy.sparse as sp\n",
    "import random, os, datetime, math\n",
    "from keras.models import load_model\n",
    "from sklearn.cluster import KMeans\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras.models import Sequential, Model, load_model, save_model\n",
    "from keras.layers.core import Dense, Lambda, Activation\n",
    "from keras.layers import Embedding, Input, Dense, Multiply, Reshape, Flatten, Concatenate\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    \"\"\" Reads in the MPD dataset, and returns a tuple of a pandas dataframe\n",
    "    and a sparse matrix of artist/pid/track_count \"\"\"\n",
    "\n",
    "    start = 0\n",
    "    numFiles = 10\n",
    "    songPlaylistArray = []\n",
    "    while numFiles:\n",
    "        path = 'data/mpd.slice.' + str(start) + \"-\" + str(start+999) + '.json'\n",
    "        d = json.load(open(path, 'r'))\n",
    "        thisSlice = pd.DataFrame.from_dict(d['playlists'], orient='columns')\n",
    "        for index, row in thisSlice.iterrows():\n",
    "            for track in row['tracks']:\n",
    "                songPlaylistArray.append([track['track_uri'], track['artist_name'], track['track_name'], row['pid']])\n",
    "        start += 1000\n",
    "        numFiles = numFiles - 1\n",
    "    #converting playlist level to track level\n",
    "    songPlaylist = pd.DataFrame(songPlaylistArray, columns=['trackid', 'artist_name', 'track_name', 'pid'])\n",
    "    songPlaylist['trackindex'] = songPlaylist['trackid'].astype('category').cat.codes\n",
    "    songPlaylist['cat_pid'] = songPlaylist['pid'].astype('category').cat.codes   \n",
    "    \n",
    "    t = json.load(open('data/challenge_set.json'))\n",
    "    challenge_df = pd.DataFrame.from_dict(t['playlists'], orient='columns')\n",
    "    challenge_df['pidcat'] = challenge_df['pid'].astype('category').cat.codes\n",
    "    m = len(songPlaylist['cat_pid'].unique())\n",
    "    n = len(songPlaylist['trackindex'].unique())\n",
    "    mat = sp.dok_matrix((m, n), dtype=np.float32)\n",
    "    for pid, trackindex in zip(songPlaylist['cat_pid'], songPlaylist['trackindex']):\n",
    "        mat[pid, trackindex] = 1.0\n",
    "    \n",
    "    return songPlaylist,challenge_df, mat\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "songPlaylist, challenge_df, mat = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking if all the playlists from the challenge set are in the training set\n",
    "c = set(challenge_df['pidcat'])\n",
    "\n",
    "s = set(songPlaylist['cat_pid'])\n",
    "\n",
    "c.issubset(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songPlaylist['cat_pid'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400817,)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songPlaylist['trackindex'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading data!\n"
     ]
    }
   ],
   "source": [
    "# full NCF model\n",
    "def get_model(num_users, num_items, latent_dim=8, dense_layers=[64, 32, 16, 8],\n",
    "              reg_layers=[0, 0, 0, 0], reg_mf=0):\n",
    "\n",
    "    # input layer\n",
    "    input_user = Input(shape=(1,), dtype='int32', name='user_input')\n",
    "    input_item = Input(shape=(1,), dtype='int32', name='item_input')\n",
    "    \n",
    "    # embedding layer\n",
    "    mf_user_embedding = Embedding(input_dim=num_users, output_dim=latent_dim,\n",
    "                        name='mf_user_embedding',\n",
    "                        embeddings_initializer='RandomNormal',\n",
    "                        embeddings_regularizer=l2(reg_mf), input_length=1)\n",
    "    mf_item_embedding = Embedding(input_dim=num_items, output_dim=latent_dim,\n",
    "                        name='mf_item_embedding',\n",
    "                        embeddings_initializer='RandomNormal',\n",
    "                        embeddings_regularizer=l2(reg_mf), input_length=1)\n",
    "    mlp_user_embedding = Embedding(input_dim=num_users, output_dim=int(dense_layers[0]/2),\n",
    "                         name='mlp_user_embedding',\n",
    "                         embeddings_initializer='RandomNormal',\n",
    "                         embeddings_regularizer=l2(reg_layers[0]), \n",
    "                         input_length=1)\n",
    "    mlp_item_embedding = Embedding(input_dim=num_items, output_dim=int(dense_layers[0]/2),\n",
    "                         name='mlp_item_embedding',\n",
    "                         embeddings_initializer='RandomNormal',\n",
    "                         embeddings_regularizer=l2(reg_layers[0]), \n",
    "                         input_length=1)\n",
    "\n",
    "    # MF latent vector\n",
    "    mf_user_latent = Flatten()(mf_user_embedding(input_user))\n",
    "    mf_item_latent = Flatten()(mf_item_embedding(input_item))\n",
    "    mf_cat_latent = Multiply()([mf_user_latent, mf_item_latent])\n",
    "\n",
    "    # MLP latent vector\n",
    "    mlp_user_latent = Flatten()(mlp_user_embedding(input_user))\n",
    "    mlp_item_latent = Flatten()(mlp_item_embedding(input_item))\n",
    "    mlp_cat_latent = Concatenate()([mlp_user_latent, mlp_item_latent])\n",
    "    \n",
    "    mlp_vector = mlp_cat_latent\n",
    "    \n",
    "    # build dense layer for model\n",
    "    for i in range(1,len(dense_layers)):\n",
    "        layer = Dense(dense_layers[i],\n",
    "                      activity_regularizer=l2(reg_layers[i]),\n",
    "                      activation='relu',\n",
    "                      name='layer%d' % i)\n",
    "        mlp_vector = layer(mlp_vector)\n",
    "\n",
    "    predict_layer = Concatenate()([mf_cat_latent, mlp_vector])\n",
    "    result = Dense(1, activation='sigmoid', \n",
    "                   kernel_initializer='lecun_uniform',name='result')\n",
    "\n",
    "    model = Model(inputs=[input_user,input_item], outputs=result(predict_layer))\n",
    "\n",
    "    return model\n",
    "\n",
    "# get the training samples\n",
    "def get_train_samples(train_mat, num_negatives):\n",
    "    user_input, item_input, labels = [], [], []\n",
    "    num_user, num_item = train_mat.shape\n",
    "    for (u, i) in train_mat.keys():\n",
    "        user_input.append(u)\n",
    "        item_input.append(i)\n",
    "        labels.append(1)\n",
    "        # negative instances\n",
    "        for t in range(num_negatives):\n",
    "            j = np.random.randint(num_item)\n",
    "            while (u, j) in train_mat.keys():\n",
    "                j = np.random.randint(num_item)\n",
    "            user_input.append(u)\n",
    "            item_input.append(j)\n",
    "            labels.append(0)\n",
    "    return user_input, item_input, labels\n",
    "\n",
    "# hyperparameters\n",
    "loaded = True\n",
    "verbose = 1\n",
    "epochs = 3   # small is due to time constraints\n",
    "batch_size = 256\n",
    "latent_dim = 8\n",
    "dense_layers = [64, 32, 16, 8]\n",
    "reg_layers = [0, 0, 0, 0]\n",
    "reg_mf = [0]\n",
    "num_negatives = 4\n",
    "learning_rate = 0.001\n",
    "learner = 'adam'\n",
    "dataset = 'spotify'\n",
    "\n",
    "# loading data\n",
    "if loaded:\n",
    "    train_mat = mat\n",
    "else:\n",
    "    train_mat = sp.load_npz('spotify_train_matrix.npz')\n",
    "    \n",
    "num_users, num_items = train_mat.shape\n",
    "print('Done loading data!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400817"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "user_input (InputLayer)         (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_input (InputLayer)         (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mlp_user_embedding (Embedding)  (None, 1, 32)        1280000     user_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "mlp_item_embedding (Embedding)  (None, 1, 32)        12848960    item_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 32)           0           mlp_user_embedding[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 32)           0           mlp_item_embedding[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 64)           0           flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mf_user_embedding (Embedding)   (None, 1, 8)         320000      user_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "mf_item_embedding (Embedding)   (None, 1, 8)         3212240     item_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer1 (Dense)                  (None, 32)           2080        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 8)            0           mf_user_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 8)            0           mf_item_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer2 (Dense)                  (None, 16)           528         layer1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 8)            0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer3 (Dense)                  (None, 8)            136         layer2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 16)           0           multiply_1[0][0]                 \n",
      "                                                                 layer3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "result (Dense)                  (None, 1)            17          concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 17,663,961\n",
      "Trainable params: 17,663,961\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tirth\\Anaconda3\\envs\\keras_gpu_dev\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "13220440/13220440 [==============================] - 966s 73us/step - loss: 0.2855 - accuracy: 0.8960\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tirth\\Anaconda3\\envs\\keras_gpu_dev\\lib\\site-packages\\keras\\callbacks\\callbacks.py:846: RuntimeWarning: Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13220440/13220440 [==============================] - 986s 75us/step - loss: 0.2126 - accuracy: 0.9228\n",
      "Epoch 3/3\n",
      "13220440/13220440 [==============================] - 980s 74us/step - loss: 0.1814 - accuracy: 0.9340\n",
      "Wall time: 49min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=2)\n",
    "# get model\n",
    "model = get_model(num_users, num_items, latent_dim, dense_layers, reg_layers, reg_mf)\n",
    "model.compile(optimizer=Adam(lr=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "    \n",
    "# train model\n",
    "# generate training instances\n",
    "user_input, item_input, labels = get_train_samples(train_mat, num_negatives)\n",
    "\n",
    "# training\n",
    "hist = model.fit([np.array(user_input), np.array(item_input)], np.array(labels), \n",
    "                 batch_size=batch_size, epochs=epochs, verbose=verbose, shuffle=True, callbacks=[es])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model file\n",
    "model_file = '%s_NCF_%d_%s.h5' % (dataset, latent_dim, str(dense_layers))\n",
    "model.save(model_file, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = json.load(open('data/challenge_set.json'))\n",
    "songPlaylistArray1 = []\n",
    "thisSlice = pd.DataFrame.from_dict(q['playlists'], orient='columns')\n",
    "for index, row in thisSlice.iterrows():\n",
    "    for track in row['tracks']:\n",
    "        songPlaylistArray1.append([track['track_uri'],row['pid']])\n",
    "songPlaylist1 = pd.DataFrame(songPlaylistArray1, columns=['trackid', 'pid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "songPlaylist1['pidcat'] = songPlaylist1['pid'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trackid</th>\n",
       "      <th>pid</th>\n",
       "      <th>pidcat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>spotify:track:66U0ASk1VHZsqIkpMjKX3B</td>\n",
       "      <td>1000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>spotify:track:5MhsZlmKJG6X5kTHkdwC4B</td>\n",
       "      <td>1000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spotify:track:0GZoB8h0kqXn7XFm4Sj06k</td>\n",
       "      <td>1000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>spotify:track:35kahykNu00FPysz3C2euR</td>\n",
       "      <td>1000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>spotify:track:3G6hD9B2ZHOsgf4WfNu7X1</td>\n",
       "      <td>1000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>spotify:track:6WQLkih8nE0JdUCEyLaGnQ</td>\n",
       "      <td>1000016</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>spotify:track:37sINbJZcFdHFAsVNsPq1i</td>\n",
       "      <td>1000016</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>spotify:track:0yhPEz5KxlDwckGJaMlZqM</td>\n",
       "      <td>1000016</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>spotify:track:5j9iuo3tMmQIfnEEQOOjxh</td>\n",
       "      <td>1000016</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>spotify:track:4eLSCSELtKxZwXnFbNLXT5</td>\n",
       "      <td>1000016</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>spotify:track:4PvD06Pmbm2rHG2JjSlElF</td>\n",
       "      <td>1000020</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>spotify:track:57nNNkgk768QVXq3uHxu5e</td>\n",
       "      <td>1000020</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>spotify:track:3y8AEUef1AVfr0npU5UOa9</td>\n",
       "      <td>1000020</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>spotify:track:5jJ69cMDMC0aeWPjZo6VP2</td>\n",
       "      <td>1000020</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>spotify:track:02b5L9jExmkRTdUTqXFzmR</td>\n",
       "      <td>1000020</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>spotify:track:6fKEplI9iN0JMHsRGQESaT</td>\n",
       "      <td>1000023</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>spotify:track:3ebXMykcMXOcLeJ9xZ17XH</td>\n",
       "      <td>1000023</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>spotify:track:1J9KJgXKFRqKGIzmJ7GjS3</td>\n",
       "      <td>1000023</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>spotify:track:6aBYg6Npa47VAqtEsLsUdz</td>\n",
       "      <td>1000023</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>spotify:track:0dA2Mk56wEzDgegdC6R17g</td>\n",
       "      <td>1000023</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 trackid      pid  pidcat\n",
       "0   spotify:track:66U0ASk1VHZsqIkpMjKX3B  1000000       0\n",
       "1   spotify:track:5MhsZlmKJG6X5kTHkdwC4B  1000000       0\n",
       "2   spotify:track:0GZoB8h0kqXn7XFm4Sj06k  1000000       0\n",
       "3   spotify:track:35kahykNu00FPysz3C2euR  1000000       0\n",
       "4   spotify:track:3G6hD9B2ZHOsgf4WfNu7X1  1000000       0\n",
       "5   spotify:track:6WQLkih8nE0JdUCEyLaGnQ  1000016       3\n",
       "6   spotify:track:37sINbJZcFdHFAsVNsPq1i  1000016       3\n",
       "7   spotify:track:0yhPEz5KxlDwckGJaMlZqM  1000016       3\n",
       "8   spotify:track:5j9iuo3tMmQIfnEEQOOjxh  1000016       3\n",
       "9   spotify:track:4eLSCSELtKxZwXnFbNLXT5  1000016       3\n",
       "10  spotify:track:4PvD06Pmbm2rHG2JjSlElF  1000020       4\n",
       "11  spotify:track:57nNNkgk768QVXq3uHxu5e  1000020       4\n",
       "12  spotify:track:3y8AEUef1AVfr0npU5UOa9  1000020       4\n",
       "13  spotify:track:5jJ69cMDMC0aeWPjZo6VP2  1000020       4\n",
       "14  spotify:track:02b5L9jExmkRTdUTqXFzmR  1000020       4\n",
       "15  spotify:track:6fKEplI9iN0JMHsRGQESaT  1000023       5\n",
       "16  spotify:track:3ebXMykcMXOcLeJ9xZ17XH  1000023       5\n",
       "17  spotify:track:1J9KJgXKFRqKGIzmJ7GjS3  1000023       5\n",
       "18  spotify:track:6aBYg6Npa47VAqtEsLsUdz  1000023       5\n",
       "19  spotify:track:0dA2Mk56wEzDgegdC6R17g  1000023       5"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songPlaylist1.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate recommendations for each pid and creating submission file\n",
    "def output(output_filename):\n",
    "    \n",
    "    model_path = 'spotify_NCF_8_[64, 32, 16, 8].h5'\n",
    "    print('using model: %s' % model_path)\n",
    "    model = load_model(model_path)\n",
    "    print('Loaded model!')\n",
    "\n",
    "    mlp_user_embedding_weights = (next(iter(filter(lambda x: x.name == 'mlp_user_embedding', model.layers))).get_weights())\n",
    "\n",
    "    \n",
    "    challenge_df['pidcat'] = challenge_df['pid'].astype('category').cat.codes\n",
    "    \n",
    "    first_line = 'team_info,team_name,main,your@email.com'\n",
    "    recs = ['']\n",
    "    start = time.time()\n",
    "    count = 1\n",
    "    with codecs.open(output_filename, \"w\") as o:\n",
    "        o.write(\"%s \\n\" %(first_line))\n",
    "        o.write(\"\\n\")\n",
    "        o.write(\"\\n\")\n",
    "        for playlist_id in challenge_df['pidcat']:\n",
    "   \n",
    "            desired_user_id = playlist_id\n",
    "            user_latent_matrix = mlp_user_embedding_weights[0]\n",
    "            one_user_vector = user_latent_matrix[desired_user_id,:]\n",
    "            one_user_vector = np.reshape(one_user_vector, (1,32))\n",
    "            #print('\\nPerforming kmeans to find the nearest users/playlists...')\n",
    "            \n",
    "            kmeans = KMeans(n_clusters=100, random_state=0, verbose=0).fit(user_latent_matrix)\n",
    "            desired_user_label = kmeans.predict(one_user_vector)\n",
    "            user_label = kmeans.labels_\n",
    "            neighbors = []\n",
    "            for user_id, user_label in enumerate(user_label):\n",
    "                if user_label == desired_user_label:\n",
    "                    neighbors.append(user_id)\n",
    "            #print('Found {0} neighbor users/playlists.'.format(len(neighbors)))\n",
    "            \n",
    "            tracks = []\n",
    "            for user_id in neighbors:\n",
    "                tracks += list(songPlaylist[songPlaylist['pid'] == int(user_id)]['trackindex'])\n",
    "            #print('Found {0} neighbor tracks from these users.'.format(len(tracks))) \n",
    "            users = np.full(len(tracks), desired_user_id, dtype='int32')\n",
    "            items = np.array(tracks, dtype='int32')\n",
    "            results = model.predict([users,items],batch_size=100, verbose=0) \n",
    "            results = results.tolist()\n",
    "            #print('Ranked the tracks!')\n",
    "\n",
    "            results_df = pd.DataFrame(np.nan, index=range(len(results)), columns=['probability','trackid'])\n",
    "            for i, prob in enumerate(results):\n",
    "                results_df.loc[i] = [prob[0], songPlaylist[songPlaylist['trackindex'] == i].iloc[0]['trackid']]\n",
    "            results_df = results_df.sort_values(by=['probability'], ascending=False).reset_index().drop(columns=['probability', 'index'])\n",
    "            \n",
    "            #dropping Duplicates.\n",
    "            results_df = results_df.drop_duplicates(subset=['trackid'], keep='first')\n",
    "            \n",
    "            set_songP = set(songPlaylist1[songPlaylist1['pidcat'] == playlist_id]['trackid'])\n",
    "\n",
    "            #Checking for seed track in the generated track list, if present, remove from the list\n",
    "            final = []\n",
    "            i = 500\n",
    "            j = 0\n",
    "            track_list = results_df['trackid']\n",
    "            while i>0:\n",
    "                if track_list[j] not in set_songP:\n",
    "                    final.append(track_list[j])    \n",
    "                    i -= 1\n",
    "                j += 1\n",
    "            final_df = pd.DataFrame(final, columns=['trackid'])\n",
    "            \n",
    "            pid = challenge_df.loc[challenge_df['pidcat'] == playlist_id]['pid'].values[0]\n",
    "            for row in final_df[\"trackid\"]:\n",
    "                recs.append(row)\n",
    "            o.write(\"%s\" %(pid))\n",
    "            recs = ', '.join(map(str, recs))\n",
    "            o.write(recs)\n",
    "            o.write(\"\\n\")\n",
    "            o.write(\"\\n\")\n",
    "            recs = ['']\n",
    "            print(f\"songs for playlist {count}\")\n",
    "            count+=1\n",
    "    logging.debug(\"generated recommendations in %0.2fs\",  time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using model: spotify_NCF_8_[64, 32, 16, 8].h5\n",
      "Loaded model!\n",
      "songs for playlist 1\n",
      "songs for playlist 2\n",
      "songs for playlist 3\n",
      "songs for playlist 4\n",
      "songs for playlist 5\n"
     ]
    }
   ],
   "source": [
    "output('data/myresults.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('if its not 9:30 am them its not complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
